{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayes_URDU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bErI-ZOnBFZ2"
      },
      "source": [
        "This is my Implementation of Naive Bayes Classifier using Bags of word appoach.\n",
        "\n",
        "I'm Using Glob library to get all the filenames from a directory, other than that i'm using Urduhack for data pre processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhTsEkZt247n",
        "outputId": "f26aba5d-46ed-4107-a5e2-fe1d78f07a9f"
      },
      "source": [
        "\n",
        "import glob #Use to get all filenames from a directory\n",
        "!pip install urduhack[tf]\n",
        "import urduhack\n",
        "urduhack.download()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: urduhack[tf] in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: tensorflow-datasets~=3.1 in /usr/local/lib/python3.7/dist-packages (from urduhack[tf]) (3.2.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from urduhack[tf]) (2019.12.20)\n",
            "Requirement already satisfied: tf2crf in /usr/local/lib/python3.7/dist-packages (from urduhack[tf]) (0.1.32)\n",
            "Requirement already satisfied: Click~=7.1 in /usr/local/lib/python3.7/dist-packages (from urduhack[tf]) (7.1.2)\n",
            "Requirement already satisfied: tensorflow~=2.2; extra == \"tf\" in /usr/local/lib/python3.7/dist-packages (from urduhack[tf]) (2.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (2.23.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (0.12.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (2.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (4.41.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (1.12.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (0.3.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (21.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (0.16.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (3.12.4)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-addons>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from tf2crf->urduhack[tf]) (0.13.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (2.5.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (0.36.2)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (2.5.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (1.12)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (1.34.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (3.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack[tf]) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack[tf]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack[tf]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack[tf]) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-datasets~=3.1->urduhack[tf]) (56.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets~=3.1->urduhack[tf]) (1.53.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons>=0.8.2->tf2crf->urduhack[tf]) (2.7.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (1.30.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (3.3.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (4.7.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (4.0.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow~=2.2; extra == \"tf\"->urduhack[tf]) (3.4.1)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4tMsthwBrdc"
      },
      "source": [
        "In this cell my implementation of Niave Bayes is written. It is quite a large code as i tried multiple approaches and they required lots of helper function. \n",
        "\n",
        "My approaches Include:\n",
        "\n",
        "On Multinomial Naive Bayes:\n",
        "\n",
        "Training and Testing without removing stopwords\n",
        "\n",
        "Training (without stopwords) and testing (with stopwords)\n",
        "\n",
        "Training (without stopwords) and testing (without stopwords)\n",
        "\n",
        "On Boolean Naive Bayes:\n",
        "\n",
        "Removing Repitition from the data\n",
        "\n",
        "\n",
        "Training and Testing without removing stopwords\n",
        "\n",
        "Training (without stopwords) and testing (with stopwords)\n",
        "\n",
        "Training (without stopwords) and testing (without stopwords)\n",
        "\n",
        "On Boolean Naive Bayes and ignoring negations as a bonus task:\n",
        "\n",
        "Training and Testing without removing stopwords\n",
        "\n",
        "Training (without stopwords) and testing (with stopwords)\n",
        "\n",
        "Training (without stopwords) and testing (without stopwords)\n",
        "\n",
        "*Functionality and working of each function is provided in its defination\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6q2cHiyDCgf"
      },
      "source": [
        "class NaiveBayes:\n",
        "  def __init__(self,DirFake,DirReal,DirFakeTest,DirRealTest,StopWordsDir):\n",
        "    '''\n",
        "    This is the constuctor, i loaded all the textfile names from the directories\n",
        "    and loaded all the stopword we were provided\n",
        "    '''\n",
        "    self.fakeDS=glob.glob(DirFake+\"/*\")\n",
        "    self.realDS=glob.glob(DirReal+\"/*\")\n",
        "    self.fakeDSTest=glob.glob(DirFakeTest+\"/*\")\n",
        "    self.realDSTest=glob.glob(DirRealTest+\"/*\")\n",
        "    self.stopWords=[]\n",
        "    with open(StopWordsDir, 'rt', encoding=\"utf-8\") as f:\n",
        "        self.stopWords=f.readlines()\n",
        "    self.stopWords=[x.strip() for x in self.stopWords]\n",
        "  def removePunctuation(self,text):# Removes Punctuation\n",
        "    from urduhack.preprocessing import remove_punctuation\n",
        "    return remove_punctuation(text)\n",
        "  def removeNumbers(self,text):# Removes Numbers\n",
        "    from urduhack.preprocessing import replace_phone_numbers\n",
        "    return replace_phone_numbers(text)\n",
        "  def removeEnglish(self,text):\n",
        "    from urduhack.preprocessing import remove_english_alphabets\n",
        "    return urduhack.preprocessing.remove_english_alphabets(text)\n",
        "  def wordTokenize(self,text):\n",
        "    from urduhack.tokenization import sentence_tokenizer\n",
        "    sentences = sentence_tokenizer(text)\n",
        "    from urduhack.tokenization import word_tokenizer\n",
        "    lX=[]\n",
        "    for sent in sentences:\n",
        "      lX.append(sent.split(' '))\n",
        "    flat_list = [item for sublist in lX for item in sublist]\n",
        "    return flat_list\n",
        "  def normalizeText(self,text):# Control Function\n",
        "    from urduhack.normalization import normalize_characters\n",
        "    x = normalize_characters(text)\n",
        "    x=self.removePunctuation(text)\n",
        "    x=self.removeNumbers(x)\n",
        "    x=self.removeEnglish(x)\n",
        "    x=self.wordTokenize(x)\n",
        "    return x\n",
        "  def loadTrainData(self):\n",
        "    '''\n",
        "    Loading Fake and Real Train data into lists of sublists where\n",
        "    each sublist contains the normalized and tokenized content of one file\n",
        "    '''\n",
        "    self.urduListFakeTrain=[]\n",
        "    for textFile in self.fakeDS:\n",
        "      urdu_text = open(textFile, 'r', encoding=\"utf8\").read()\n",
        "      self.urduListFakeTrain.append(self.normalizeText(urdu_text))\n",
        "    self.urduListRealTrain=[]\n",
        "    for textFile in self.realDS:\n",
        "      urdu_text = open(textFile, 'r', encoding=\"utf8\").read()\n",
        "      self.urduListRealTrain.append(self.normalizeText(urdu_text))\n",
        "  def train(self):\n",
        "    '''\n",
        "    First i created a FakeVocabularly and a Real Vocabularly. Then from those\n",
        "    i created a global vocab named myVocab after that i created 2 bag of words which contained\n",
        "    words from myVocab, one was for fakeData and one for realData.\n",
        "    After that i removed stopwords from myVocab and recreated 2 more bag of words in \n",
        "    the same way i did before.\n",
        "    '''\n",
        "    self.fakeVocab=[]\n",
        "    for line in self.urduListFakeTrain:\n",
        "      for word in line:\n",
        "        self.fakeVocab.append(word)\n",
        "    self.realVocab=[]\n",
        "    for line in self.urduListRealTrain:\n",
        "      for word in line:\n",
        "        self.realVocab.append(word)\n",
        "    self.myVocab=self.fakeVocab+self.realVocab\n",
        "    self.fakeBagofWords={}\n",
        "    for word in self.myVocab:\n",
        "      if word in self.fakeVocab:\n",
        "        if word not in self.fakeBagofWords:\n",
        "          self.fakeBagofWords[word]=1\n",
        "        else:\n",
        "          self.fakeBagofWords[word]+=1\n",
        "      else:\n",
        "        self.fakeBagofWords[word]=1\n",
        "    self.realBagofWords={}\n",
        "    for word in self.myVocab:\n",
        "      if word in self.realVocab:\n",
        "        if word not in self.realBagofWords:\n",
        "          self.realBagofWords[word]=1\n",
        "        else:\n",
        "          self.realBagofWords[word]+=1\n",
        "      else:\n",
        "        self.realBagofWords[word]=1\n",
        "    ##removing Stop Words from Vocab\n",
        "    self.myVocabStop=[]\n",
        "    for word in self.myVocab:\n",
        "      if word not in self.stopWords:\n",
        "        self.myVocabStop.append(word)\n",
        "    self.fakeBagofWordStop={}\n",
        "    for word in self.myVocabStop:\n",
        "      if word in self.fakeVocab:\n",
        "        if word not in self.fakeBagofWordStop:\n",
        "          self.fakeBagofWordStop[word]=1\n",
        "        else:\n",
        "          self.fakeBagofWordStop[word]+=1\n",
        "      else:\n",
        "        self.fakeBagofWordStop[word]=1\n",
        "    self.realBagofWordStop={}\n",
        "    for word in self.myVocabStop:\n",
        "      if word in self.realVocab:\n",
        "        if word not in self.realBagofWordStop:\n",
        "          self.realBagofWordStop[word]=1\n",
        "        else:\n",
        "          self.realBagofWordStop[word]+=1\n",
        "      else:\n",
        "        self.realBagofWordStop[word]=1\n",
        "  def checkVar(self): #Just a function to check data's length\n",
        "    print(len(self.myVocab))\n",
        "    print(len(self.myVocabStop))\n",
        "    print(len(self.fakeBagofWords))\n",
        "    print(len(self.realBagofWords))\n",
        "    print(len(self.fakeBagofWordStop))\n",
        "    print(len(self.realBagofWordStop))\n",
        "  def loadtestData(self):\n",
        "    '''\n",
        "    I'm loading test data the same way i loaded trained data into to seperate\n",
        "    list of sublist names of which can be easily recognised\n",
        "    '''\n",
        "    self.urduListFakeTest=[]\n",
        "    for textFile in self.fakeDSTest:\n",
        "      urdu_text = open(textFile, 'r', encoding=\"utf8\").read()\n",
        "      self.urduListFakeTest.append(self.normalizeText(urdu_text))\n",
        "    self.urduListRealTest=[]\n",
        "    for textFile in self.realDSTest:\n",
        "      urdu_text = open(textFile, 'r', encoding=\"utf8\").read()\n",
        "      self.urduListRealTest.append(self.normalizeText(urdu_text))\n",
        "    self.urduListFakeTestStop=[]\n",
        "    self.urduListRealTestStop=[]\n",
        "    for line in self.urduListFakeTest:\n",
        "      lX=[]\n",
        "      for word in line:\n",
        "        if word not in self.stopWords:\n",
        "          lX.append(word)\n",
        "      self.urduListFakeTestStop.append(lX)\n",
        "    for line in self.urduListRealTest:\n",
        "      lX=[]\n",
        "      for word in line:\n",
        "        if word not in self.stopWords:\n",
        "          lX.append(word)\n",
        "      self.urduListRealTestStop.append(lX)\n",
        "  def testNoStopwordRemoval(self):\n",
        "    '''\n",
        "    This is the test function where i use the niave bayes formula to predict\n",
        "    a class for the data and then i check n.o of correct classification and n.o.\n",
        "    of incorrect classification to calculate accuracy.\n",
        "    '''\n",
        "    TotalFakeNews=len(self.urduListFakeTest)\n",
        "    TotalRealNews=len(self.urduListRealTest)\n",
        "    fakeClassified=0\n",
        "    realClassified=0\n",
        "    for line in self.urduListFakeTest:\n",
        "      pReal=1\n",
        "      pFake=1\n",
        "      for eachWord in line:\n",
        "        if eachWord in self.fakeBagofWords:\n",
        "          probFake=self.fakeBagofWords[eachWord]/sum(self.fakeBagofWords.values())\n",
        "        if eachWord in self.realBagofWords:\n",
        "          probReal=self.realBagofWords[eachWord]/sum(self.realBagofWords.values())\n",
        "        pReal*=probReal\n",
        "        pFake*=probFake\n",
        "      if pReal*(TotalRealNews/TotalRealNews+TotalFakeNews)>=pFake*(TotalFakeNews/TotalRealNews+TotalFakeNews):\n",
        "        realClassified+=1\n",
        "      else:\n",
        "        fakeClassified+=1\n",
        "    fakeClassified2=0\n",
        "    realClassified2=0\n",
        "    for line in self.urduListRealTest:\n",
        "      pReal=1\n",
        "      pFake=1\n",
        "      for eachWord in line:\n",
        "        if eachWord in self.fakeBagofWords:\n",
        "          probFake=self.fakeBagofWords[eachWord]/sum(self.fakeBagofWords.values())\n",
        "        if eachWord in self.realBagofWords:\n",
        "          probReal=self.realBagofWords[eachWord]/sum(self.realBagofWords.values())\n",
        "        pReal*=probReal\n",
        "        pFake*=probFake\n",
        "      if pReal*(TotalRealNews/TotalRealNews+TotalFakeNews)>=pFake*(TotalFakeNews/TotalRealNews+TotalFakeNews):\n",
        "        realClassified2+=1\n",
        "      else:\n",
        "        fakeClassified2+=1\n",
        "    return \"Total Accuracy With Stopwords Present: \",((fakeClassified+realClassified2)/(TotalRealNews+TotalFakeNews))*100 \n",
        "  def testStopwordRemoval(self):\n",
        "    '''\n",
        "    This is the test function where i use the niave bayes formula on train data with stop words\n",
        "    removed to predict a class for the data and then i check n.o of correct classification and n.o.\n",
        "    of incorrect classification to calculate accuracy.\n",
        "    '''\n",
        "    TotalFakeNews=len(self.urduListFakeTest)\n",
        "    TotalRealNews=len(self.urduListRealTest)\n",
        "    fakeClassified=0\n",
        "    realClassified=0\n",
        "    for line in self.urduListFakeTest:\n",
        "      pReal=1\n",
        "      pFake=1\n",
        "      for eachWord in line:\n",
        "        if eachWord in self.fakeBagofWordStop:\n",
        "          probFake=self.fakeBagofWordStop[eachWord]/sum(self.fakeBagofWordStop.values())\n",
        "        if eachWord in self.realBagofWordStop:\n",
        "          probReal=self.realBagofWordStop[eachWord]/sum(self.realBagofWordStop.values())\n",
        "        pReal*=probReal\n",
        "        pFake*=probFake\n",
        "      if pReal*(TotalRealNews/TotalRealNews+TotalFakeNews)>=pFake*(TotalFakeNews/TotalRealNews+TotalFakeNews):\n",
        "        realClassified+=1\n",
        "      else:\n",
        "        fakeClassified+=1\n",
        "    fakeClassified2=0\n",
        "    realClassified2=0\n",
        "    for line in self.urduListRealTest:\n",
        "      pReal=1\n",
        "      pFake=1\n",
        "      for eachWord in line:\n",
        "        if eachWord in self.fakeBagofWordStop:\n",
        "          probFake=self.fakeBagofWordStop[eachWord]/sum(self.fakeBagofWordStop.values())\n",
        "        if eachWord in self.realBagofWordStop:\n",
        "          probReal=self.realBagofWordStop[eachWord]/sum(self.realBagofWordStop.values())\n",
        "        pReal*=probReal\n",
        "        pFake*=probFake\n",
        "      if pReal*(TotalRealNews/TotalRealNews+TotalFakeNews)>=pFake*(TotalFakeNews/TotalRealNews+TotalFakeNews):\n",
        "        realClassified2+=1\n",
        "      else:\n",
        "        fakeClassified2+=1\n",
        "    return \"Total Accuracy after Removing Stop Words: \",((fakeClassified+realClassified2)/(TotalRealNews+TotalFakeNews))*100\n",
        "  def testNoStopTrainAndTest(self): \n",
        "    '''\n",
        "    This is the test function where i use the niave bayes formula on train data with stop words\n",
        "    removed from train and as well as test data, to predict a class for the data and then i\n",
        "    check n.o of correct classification and n.o. of incorrect classification to calculate accuracy.\n",
        "    '''\n",
        "    TotalFakeNews=len(self.urduListFakeTest)\n",
        "    TotalRealNews=len(self.urduListRealTest)\n",
        "    fakeClassified=0\n",
        "    realClassified=0\n",
        "    for line in self.urduListFakeTestStop:\n",
        "      pReal=1\n",
        "      pFake=1\n",
        "      for eachWord in line:\n",
        "        if eachWord in self.fakeBagofWordStop:\n",
        "          probFake=self.fakeBagofWordStop[eachWord]/sum(self.fakeBagofWordStop.values())\n",
        "        if eachWord in self.realBagofWordStop:\n",
        "          probReal=self.realBagofWordStop[eachWord]/sum(self.realBagofWordStop.values())\n",
        "        pReal*=probReal\n",
        "        pFake*=probFake\n",
        "      if pReal*(TotalRealNews/TotalRealNews)>=pFake*(TotalFakeNews/TotalRealNews+TotalFakeNews):\n",
        "        realClassified+=1\n",
        "      else:\n",
        "        fakeClassified+=1\n",
        "    fakeClassified2=0\n",
        "    realClassified2=0\n",
        "    for line in self.urduListRealTestStop:\n",
        "      pReal=1\n",
        "      pFake=1\n",
        "      for eachWord in line:\n",
        "        if eachWord in self.fakeBagofWordStop:\n",
        "          probFake=self.fakeBagofWordStop[eachWord]/sum(self.fakeBagofWordStop.values())\n",
        "        if eachWord in self.realBagofWordStop:\n",
        "          probReal=self.realBagofWordStop[eachWord]/sum(self.realBagofWordStop.values())\n",
        "        pReal*=probReal\n",
        "        pFake*=probFake\n",
        "      if pReal*(TotalRealNews/TotalRealNews+TotalFakeNews)>=pFake*(TotalFakeNews/TotalRealNews+TotalFakeNews):\n",
        "        realClassified2+=1\n",
        "      else:\n",
        "        fakeClassified2+=1\n",
        "    return \"Total Accuracy after Removing Stop Words From Train and Test: \",((fakeClassified+realClassified2)/(TotalRealNews+TotalFakeNews))*100\n",
        "  def test(self,negatiotR):\n",
        "    '''\n",
        "    This test function calls all other test functions.\n",
        "    '''\n",
        "    self.loadtestData()\n",
        "    if negatiotR:\n",
        "      self.removeNegationTest()\n",
        "    print(self.testNoStopwordRemoval())\n",
        "    print(self.testStopwordRemoval())\n",
        "    print(self.testNoStopTrainAndTest())\n",
        "  def removeNegationTest(self):\n",
        "    '''\n",
        "    Removing common urdu negation words in this function from test data\n",
        "    '''\n",
        "    urduListFakeTestX=[]\n",
        "    urduListRealTestX=[]\n",
        "    urduListFakeTestStopX=[]\n",
        "    urduListRealTestStopX=[]\n",
        "    import copy\n",
        "    urduNegation=['انکار',' نہیں','مت','عدم','فقدان','انکار','نفی','تردید','نہ','سلب','معدوم']\n",
        "    for line in self.urduListFakeTest:\n",
        "      lX=[]\n",
        "      for word in line:\n",
        "        if word not in urduNegation:\n",
        "          lX.append(word)\n",
        "      urduListFakeTestX.append(lX)\n",
        "    for line in self.urduListRealTest:\n",
        "      lX=[]\n",
        "      for word in line:\n",
        "        if word not in urduNegation:\n",
        "          lX.append(word)\n",
        "      urduListRealTestX.append(lX)\n",
        "    for line in self.urduListFakeTestStop:\n",
        "      lX=[]\n",
        "      for word in line:\n",
        "        if word not in urduNegation:\n",
        "          lX.append(word)\n",
        "      urduListFakeTestStopX.append(lX)\n",
        "    for line in self.urduListRealTestStop:\n",
        "      lX=[]\n",
        "      for word in line:\n",
        "        if word not in urduNegation:\n",
        "          lX.append(word)\n",
        "      urduListRealTestStopX.append(lX)\n",
        "    self.urduListFakeTest=copy.deepcopy(urduListFakeTestX)\n",
        "    self.urduListRealTest=copy.deepcopy(urduListRealTestX)\n",
        "    self.urduListFakeTestStop=copy.deepcopy(urduListFakeTestStopX)\n",
        "    self.urduListRealTestStop=copy.deepcopy(urduListRealTestStopX)\n",
        "  def makeTrainDataBooleanNB(self):\n",
        "    '''\n",
        "    Removing repeating words from train data for Multnomial Naive Bayes Classification. Using Sets.\n",
        "    '''\n",
        "    #Training Data Shouldn't have Repetion, remove repitition from individual files\n",
        "    self.urduListFakeTrain=[]\n",
        "    for textFile in self.fakeDS:\n",
        "      urdu_text = open(textFile, 'r', encoding=\"utf8\").read()\n",
        "      self.urduListFakeTrain.append(self.normalizeText(urdu_text))\n",
        "    self.urduListRealTrain=[]\n",
        "    for textFile in self.realDS:\n",
        "      urdu_text = open(textFile, 'r', encoding=\"utf8\").read()\n",
        "      self.urduListRealTrain.append(self.normalizeText(urdu_text))\n",
        "    lXX=[]\n",
        "    for line in self.urduListFakeTrain:\n",
        "      lX=set(line)\n",
        "      line=list(lX)\n",
        "      lXX.append(line)\n",
        "    lYY=[]\n",
        "    for line in self.urduListRealTrain:\n",
        "      lX=set(line)\n",
        "      line=list(lX)\n",
        "      lYY.append(line)\n",
        "    import copy\n",
        "    self.urduListFakeTrain=copy.deepcopy(lXX)\n",
        "    self.urduListRealTrain=copy.deepcopy(lYY)\n",
        "  def removeNegationTrain(self):\n",
        "    '''\n",
        "    Removing negation from train data\n",
        "    '''\n",
        "    urduListFakeTrainX=[]\n",
        "    urduListRealTrainX=[]\n",
        "    import copy\n",
        "    urduNegation=['انکار',' نہیں','مت','عدم','فقدان','انکار','نفی','تردید','نہ','سلب','معدوم']\n",
        "    for line in self.urduListFakeTrain:\n",
        "      lX=[]\n",
        "      for word in line:\n",
        "        if word not in urduNegation:\n",
        "          lX.append(word)\n",
        "      urduListFakeTrainX.append(lX)\n",
        "    for line in self.urduListRealTrain:\n",
        "      lX=[]\n",
        "      for word in line:\n",
        "        if word not in urduNegation:\n",
        "          lX.append(word)\n",
        "      urduListRealTrainX.append(lX)\n",
        "    self.urduListRealTrain=copy.deepcopy(urduListRealTrainX)\n",
        "    self.urduListFakeTrain=copy.deepcopy(urduListFakeTrainX)\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kADQyjvVS3K1"
      },
      "source": [
        "Lets call the Multinomial Naive Bayes Classifier without removing Stopwords then removing them from train data and them removing them from test data as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHXyNNidIYJK",
        "outputId": "ff6bb412-fadd-4d67-86a8-631ca85d2343"
      },
      "source": [
        "Dir1=\"/content/drive/My Drive/data/Train/Fake\"\n",
        "Dir2=\"/content/drive/My Drive/data/Train/Real\"\n",
        "Dir3=\"/content/drive/My Drive/data/Test/Fake\"\n",
        "Dir4=\"/content/drive/My Drive/data/Test/Real\"\n",
        "Dir5='/content/drive/My Drive/data/stopwords-ur.txt'\n",
        "ClassifierX=NaiveBayes(Dir1,Dir2,Dir3,Dir4,Dir5)\n",
        "ClassifierX.loadTrainData()\n",
        "ClassifierX.train()\n",
        "negatiotR=False\n",
        "ClassifierX.test(negatiotR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Total Accuracy With Stopwords Present: ', 57.63358778625955)\n",
            "('Total Accuracy after Removing Stop Words: ', 58.396946564885496)\n",
            "('Total Accuracy after Removing Stop Words From Train and Test: ', 59.92366412213741)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi2kkQpATBis"
      },
      "source": [
        "Lets call the Boolean Naive Bayes Classifier without removing Stopwords then removing them from train data and them removing them from test data as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8o_Nrf1_pWK",
        "outputId": "0b0500c7-2615-4fd5-bd0d-de3d64fba2a8"
      },
      "source": [
        "ClassifierX.makeTrainDataBooleanNB()\n",
        "ClassifierX.train()\n",
        "negatiotR=False\n",
        "ClassifierX.test(negatiotR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Total Accuracy With Stopwords Present: ', 58.396946564885496)\n",
            "('Total Accuracy after Removing Stop Words: ', 58.396946564885496)\n",
            "('Total Accuracy after Removing Stop Words From Train and Test: ', 60.30534351145038)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x2PADPoTStz"
      },
      "source": [
        "Lets call the Boolean Naive Bayes Classifier while taking care of negation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y1aOI-E_p0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c135ca-81e8-4d8f-8d50-4386eb5da067"
      },
      "source": [
        "ClassifierX.removeNegationTrain()\n",
        "ClassifierX.train()\n",
        "negatiotR=True\n",
        "ClassifierX.test(negatiotR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Total Accuracy With Stopwords Present: ', 58.396946564885496)\n",
            "('Total Accuracy after Removing Stop Words: ', 58.396946564885496)\n",
            "('Total Accuracy after Removing Stop Words From Train and Test: ', 60.30534351145038)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}